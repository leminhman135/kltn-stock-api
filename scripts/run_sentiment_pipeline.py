"""
Script cháº¡y Sentiment Pipeline Ä‘áº§y Ä‘á»§ cho mÃ£ cá»• phiáº¿u

Usage:
    python scripts/run_sentiment_pipeline.py VNM --days 30
    python scripts/run_sentiment_pipeline.py --all --days 7
"""

import argparse
import sys
import os
from datetime import datetime

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import pandas as pd
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_pipeline_for_symbol(symbol: str, days: int = 30, save_to_db: bool = False):
    """
    Cháº¡y sentiment pipeline cho má»™t mÃ£ cá»• phiáº¿u
    
    7 BÆ°á»›c:
    1. Thu tháº­p tin tá»©c
    2. LÃ m sáº¡ch vÄƒn báº£n
    3. Tokenization
    4. Embedding
    5. Dá»± Ä‘oÃ¡n sentiment
    6. Chuyá»ƒn vá» dáº¡ng sá»‘
    7. Export káº¿t quáº£
    """
    logger.info("="*80)
    logger.info(f"ğŸš€ SENTIMENT PIPELINE - {symbol}")
    logger.info("="*80)
    
    from src.hybrid_sentiment import EnhancedSentimentPipeline
    from src.news_service import news_service
    
    # Initialize pipeline
    pipeline = EnhancedSentimentPipeline(use_finbert=False)
    
    # Step 1: Collect news
    logger.info(f"\nğŸ“° BÆ°á»›c 1: Thu tháº­p tin tá»©c cho {symbol}")
    news_articles = news_service.get_all_news(symbol=symbol, limit=100)
    
    if not news_articles:
        logger.warning(f"âŒ KhÃ´ng cÃ³ tin tá»©c cho {symbol}")
        return None
    
    # Convert to DataFrame
    news_data = []
    for article in news_articles:
        news_data.append({
            'date': article.published_at,
            'symbol': symbol,
            'title': article.title,
            'summary': article.summary,
            'text': f"{article.title} {article.summary}",
            'url': article.url,
            'source': article.source
        })
    
    news_df = pd.DataFrame(news_data)
    news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')
    
    # Filter by days
    from datetime import timedelta
    cutoff = datetime.now() - timedelta(days=days)
    news_df = news_df[news_df['date'] >= cutoff]
    
    logger.info(f"âœ“ Thu tháº­p {len(news_df)} tin tá»©c trong {days} ngÃ y qua")
    
    if len(news_df) == 0:
        logger.warning("âŒ KhÃ´ng cÃ³ tin tá»©c gáº§n Ä‘Ã¢y")
        return None
    
    # Steps 2-6: Process
    logger.info(f"\nğŸ”„ BÆ°á»›c 2-6: Xá»­ lÃ½ & phÃ¢n tÃ­ch sentiment")
    news_analyzed, daily_sentiment = pipeline.process_news_dataframe(news_df, text_col='text')
    
    # Display results
    logger.info(f"\n" + "="*80)
    logger.info("ğŸ“Š Káº¾T QUáº¢ PHÃ‚N TÃCH")
    logger.info("="*80)
    
    # Overall stats
    sentiment_counts = news_analyzed['sentiment'].value_counts()
    logger.info(f"\nğŸ“ˆ Tá»•ng há»£p toÃ n bá»™ tin tá»©c:")
    for sentiment, count in sentiment_counts.items():
        pct = count / len(news_analyzed) * 100
        logger.info(f"  {sentiment.upper()}: {count} tin ({pct:.1f}%)")
    
    avg_score = news_analyzed['sentiment_score'].mean()
    logger.info(f"\nğŸ’¯ Äiá»ƒm sentiment trung bÃ¬nh: {avg_score:.3f}")
    
    if avg_score > 0.2:
        logger.info(f"  â†’ ğŸŸ¢ TIN Tá»¨C TÃCH Cá»°C cho {symbol}")
    elif avg_score < -0.2:
        logger.info(f"  â†’ ğŸ”´ TIN Tá»¨C TIÃŠU Cá»°C cho {symbol}")
    else:
        logger.info(f"  â†’ ğŸŸ¡ TIN Tá»¨C TRUNG Láº¬P cho {symbol}")
    
    # Daily sentiment
    logger.info(f"\nğŸ“… Sentiment theo ngÃ y (gáº§n nháº¥t):")
    logger.info(daily_sentiment.head(10).to_string())
    
    # Step 7: Save results
    logger.info(f"\nğŸ’¾ BÆ°á»›c 7: LÆ°u káº¿t quáº£")
    
    # Save to CSV
    output_dir = "data/sentiment_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    news_file = f"{output_dir}/{symbol}_news_{timestamp}.csv"
    news_analyzed.to_csv(news_file, index=False, encoding='utf-8-sig')
    logger.info(f"  âœ“ News analysis: {news_file}")
    
    daily_file = f"{output_dir}/{symbol}_daily_{timestamp}.csv"
    daily_sentiment.to_csv(daily_file, index=False, encoding='utf-8-sig')
    logger.info(f"  âœ“ Daily sentiment: {daily_file}")
    
    # Save to database (optional)
    if save_to_db:
        logger.info(f"\nğŸ’¾ LÆ°u vÃ o database...")
        try:
            save_to_database(news_analyzed, daily_sentiment, symbol)
            logger.info(f"  âœ“ Database updated")
        except Exception as e:
            logger.error(f"  âŒ Database error: {e}")
    
    logger.info(f"\nâœ… HOÃ€N THÃ€NH pipeline cho {symbol}")
    
    return {
        'symbol': symbol,
        'news_count': len(news_analyzed),
        'days': days,
        'avg_sentiment_score': avg_score,
        'sentiment_distribution': sentiment_counts.to_dict(),
        'news_file': news_file,
        'daily_file': daily_file
    }


def save_to_database(news_df: pd.DataFrame, daily_df: pd.DataFrame, symbol: str):
    """
    LÆ°u káº¿t quáº£ vÃ o database
    
    Tables:
    - analyzed_news: Chi tiáº¿t tá»«ng tin
    - daily_sentiment: Tá»•ng há»£p theo ngÃ y
    """
    try:
        from sqlalchemy import create_engine
        import os
        
        db_url = os.getenv('DATABASE_URL')
        if not db_url:
            logger.warning("DATABASE_URL not found")
            return
        
        engine = create_engine(db_url)
        
        # Save news analysis
        news_df.to_sql(
            'analyzed_news',
            engine,
            if_exists='append',
            index=False,
            method='multi'
        )
        
        # Save daily sentiment
        daily_df.to_sql(
            'daily_sentiment',
            engine,
            if_exists='append',
            index=False,
            method='multi'
        )
        
        logger.info(f"âœ“ Saved to database")
        
    except Exception as e:
        logger.error(f"Database save error: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description='Run Sentiment Analysis Pipeline')
    
    parser.add_argument('symbol', nargs='?', type=str, help='Stock symbol (e.g., VNM, VIC)')
    parser.add_argument('--all', action='store_true', help='Run for all major symbols')
    parser.add_argument('--days', type=int, default=30, help='Number of days to analyze')
    parser.add_argument('--db', action='store_true', help='Save to database')
    
    args = parser.parse_args()
    
    # List of major symbols
    major_symbols = [
        'VNM', 'VIC', 'VHM', 'HPG', 'FPT', 'MWG', 
        'VCB', 'BID', 'CTG', 'TCB', 'MBB',
        'MSN', 'SAB', 'PLX', 'VJC', 'SSI'
    ]
    
    if args.all:
        logger.info(f"ğŸ”„ Running pipeline for {len(major_symbols)} symbols")
        results = []
        
        for symbol in major_symbols:
            try:
                result = run_pipeline_for_symbol(symbol, days=args.days, save_to_db=args.db)
                if result:
                    results.append(result)
            except Exception as e:
                logger.error(f"Error processing {symbol}: {e}")
        
        # Summary
        logger.info(f"\n" + "="*80)
        logger.info(f"ğŸ“Š Tá»”NG Káº¾T")
        logger.info("="*80)
        logger.info(f"Processed: {len(results)} symbols")
        
        for result in results:
            logger.info(f"  {result['symbol']}: {result['news_count']} news, "
                       f"avg_score={result['avg_sentiment_score']:.3f}")
    
    elif args.symbol:
        run_pipeline_for_symbol(args.symbol.upper(), days=args.days, save_to_db=args.db)
    
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
