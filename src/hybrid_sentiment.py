"""
Hybrid Sentiment Analyzer - K·∫øt h·ª£p Keyword-based (ti·∫øng Vi·ªát) + FinBERT (ti·∫øng Anh)

Chi·∫øn l∆∞·ª£c:
1. D√πng keyword-based cho tin ti·∫øng Vi·ªát (nhanh, ch√≠nh x√°c h∆°n)
2. D√πng FinBERT cho tin ti·∫øng Anh (n·∫øu c√≥)
3. D√πng translation n·∫øu c·∫ßn ph√¢n t√≠ch s√¢u
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import logging
from src.news_service import SentimentAnalyzer as KeywordAnalyzer

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    HAS_FINBERT = True
except ImportError:
    HAS_FINBERT = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HybridSentimentAnalyzer:
    """
    Hybrid Sentiment Analyzer cho ti·∫øng Vi·ªát
    
    Method 1 (Primary): Keyword-based - Ch√≠nh x√°c cao cho ti·∫øng Vi·ªát
    Method 2 (Fallback): FinBERT - Cho tin ti·∫øng Anh
    """
    
    def __init__(self, use_finbert: bool = False):
        """
        Args:
            use_finbert: C√≥ s·ª≠ d·ª•ng FinBERT kh√¥ng (t·ªën RAM h∆°n)
        """
        logger.info("üöÄ Kh·ªüi t·∫°o Hybrid Sentiment Analyzer")
        
        # Primary: Keyword-based cho ti·∫øng Vi·ªát
        self.keyword_analyzer = KeywordAnalyzer()
        logger.info("‚úì Keyword-based analyzer ready (Vietnamese)")
        
        # Optional: FinBERT cho ti·∫øng Anh
        self.finbert = None
        self.finbert_tokenizer = None
        self.device = None
        
        if use_finbert and HAS_FINBERT:
            try:
                self.finbert_tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')
                self.finbert = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')
                self.finbert.eval()
                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.finbert.to(self.device)
                logger.info(f"‚úì FinBERT ready on {self.device} (English)")
            except Exception as e:
                logger.warning(f"Failed to load FinBERT: {e}")
                self.finbert = None
    
    def _is_vietnamese(self, text: str) -> bool:
        """
        Ki·ªÉm tra vƒÉn b·∫£n c√≥ ph·∫£i ti·∫øng Vi·ªát kh√¥ng
        
        Simple heuristic: T√¨m c√°c k√Ω t·ª± c√≥ d·∫•u ti·∫øng Vi·ªát
        """
        vietnamese_chars = '√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë'
        vietnamese_chars += vietnamese_chars.upper()
        
        for char in vietnamese_chars:
            if char in text:
                return True
        
        return False
    
    def analyze(self, text: str, method: str = 'auto') -> Dict:
        """
        Ph√¢n t√≠ch sentiment v·ªõi hybrid approach
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
            method: 'auto', 'keyword', 'finbert'
        
        Returns:
            {
                'sentiment': str,
                'positive': float,
                'negative': float,
                'neutral': float,
                'sentiment_score': float,  # [-1, 1]
                'confidence': float,
                'method': str,
                'explanation': str
            }
        """
        if not text or not isinstance(text, str):
            return self._neutral_result(method='none')
        
        # Auto-detect language
        if method == 'auto':
            is_vietnamese = self._is_vietnamese(text)
            method = 'keyword' if is_vietnamese else 'finbert'
        
        # Method 1: Keyword-based (fast, accurate for Vietnamese)
        if method == 'keyword':
            sentiment, score, impact = self.keyword_analyzer.analyze(text)
            
            # Convert to standard format
            return {
                'sentiment': sentiment.value,
                'positive': max(0, score),
                'negative': abs(min(0, score)),
                'neutral': 1 - abs(score),
                'sentiment_score': score,
                'confidence': abs(score),
                'method': 'keyword-based',
                'explanation': impact
            }
        
        # Method 2: FinBERT (for English)
        elif method == 'finbert' and self.finbert is not None:
            return self._analyze_with_finbert(text)
        
        # Fallback: keyword
        else:
            sentiment, score, impact = self.keyword_analyzer.analyze(text)
            return {
                'sentiment': sentiment.value,
                'positive': max(0, score),
                'negative': abs(min(0, score)),
                'neutral': 1 - abs(score),
                'sentiment_score': score,
                'confidence': abs(score),
                'method': 'keyword-based (fallback)',
                'explanation': impact
            }
    
    def _analyze_with_finbert(self, text: str) -> Dict:
        """Ph√¢n t√≠ch v·ªõi FinBERT"""
        try:
            with torch.no_grad():
                inputs = self.finbert_tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=512,
                    padding=True
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                outputs = self.finbert(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                probs = probs.cpu().numpy()[0]
            
            # FinBERT output: [positive, negative, neutral]
            positive = float(probs[0])
            negative = float(probs[1])
            neutral = float(probs[2])
            
            # Determine sentiment
            scores = {'positive': positive, 'negative': negative, 'neutral': neutral}
            sentiment = max(scores, key=scores.get)
            
            # Sentiment score [-1, 1]
            sentiment_score = positive - negative
            
            # Explanation
            if abs(sentiment_score) > 0.5:
                explanation = f"FinBERT high confidence: {sentiment}"
            elif abs(sentiment_score) > 0.2:
                explanation = f"FinBERT moderate confidence: {sentiment}"
            else:
                explanation = f"FinBERT low confidence: neutral"
            
            return {
                'sentiment': sentiment,
                'positive': positive,
                'negative': negative,
                'neutral': neutral,
                'sentiment_score': sentiment_score,
                'confidence': scores[sentiment],
                'method': 'finbert',
                'explanation': explanation
            }
        
        except Exception as e:
            logger.error(f"FinBERT error: {e}")
            return self._neutral_result(method='finbert-error')
    
    def _neutral_result(self, method: str) -> Dict:
        """Return neutral result"""
        return {
            'sentiment': 'neutral',
            'positive': 0.0,
            'negative': 0.0,
            'neutral': 1.0,
            'sentiment_score': 0.0,
            'confidence': 1.0,
            'method': method,
            'explanation': 'No analysis'
        }
    
    def analyze_batch(self, texts: List[str], method: str = 'auto') -> List[Dict]:
        """
        Ph√¢n t√≠ch nhi·ªÅu vƒÉn b·∫£n
        
        T·ª± ƒë·ªông ph√¢n lo·∫°i Vi·ªát/Anh v√† d√πng method ph√π h·ª£p
        """
        results = []
        
        for text in texts:
            result = self.analyze(text, method=method)
            results.append(result)
        
        return results
    
    def analyze_dataframe(self, df: pd.DataFrame, text_col: str = 'text') -> pd.DataFrame:
        """
        Ph√¢n t√≠ch sentiment cho DataFrame
        
        Adds columns: sentiment, positive, negative, neutral, sentiment_score, 
                     confidence, method, explanation
        """
        logger.info(f"üîç Analyzing sentiment for {len(df)} texts")
        
        df_result = df.copy()
        
        texts = df[text_col].fillna('').tolist()
        sentiments = self.analyze_batch(texts)
        
        # Add columns
        for key in sentiments[0].keys():
            df_result[key] = [s[key] for s in sentiments]
        
        # Statistics
        method_counts = df_result['method'].value_counts()
        logger.info(f"‚úì Methods used: {method_counts.to_dict()}")
        
        sentiment_counts = df_result['sentiment'].value_counts()
        logger.info(f"‚úì Sentiments: {sentiment_counts.to_dict()}")
        
        return df_result
    
    def aggregate_by_date(self, df: pd.DataFrame, date_col: str = 'date',
                         symbol_col: str = 'symbol') -> pd.DataFrame:
        """
        T·ªïng h·ª£p sentiment theo ng√†y
        
        Returns:
            DataFrame v·ªõi daily sentiment scores
        """
        logger.info(f"üìä Aggregating sentiment by date")
        
        # Ensure date column
        df['date'] = pd.to_datetime(df[date_col])
        
        # Group by date + symbol
        agg_dict = {
            'sentiment_score': ['mean', 'std', 'min', 'max'],
            'positive': 'mean',
            'negative': 'mean',
            'neutral': 'mean',
            'confidence': 'mean',
            'sentiment': lambda x: x.mode()[0] if len(x) > 0 else 'neutral'
        }
        
        if symbol_col in df.columns:
            group_cols = ['date', symbol_col]
        else:
            group_cols = ['date']
        
        aggregated = df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Flatten columns
        aggregated.columns = [
            group_cols[0], group_cols[1] if len(group_cols) > 1 else None,
            'daily_sentiment_mean', 'daily_sentiment_std',
            'daily_sentiment_min', 'daily_sentiment_max',
            'daily_positive', 'daily_negative', 'daily_neutral',
            'daily_confidence', 'daily_sentiment_mode'
        ]
        aggregated.columns = [c for c in aggregated.columns if c is not None]
        
        # Count news
        news_count = df.groupby(group_cols).size().reset_index(name='news_count')
        aggregated = aggregated.merge(news_count, on=group_cols)
        
        logger.info(f"‚úì Aggregated {len(aggregated)} days")
        
        return aggregated


# ============ Integration v·ªõi Pipeline c≈© ============

class EnhancedSentimentPipeline:
    """
    Pipeline n√¢ng c·∫•p v·ªõi Hybrid Analyzer
    
    7 B∆∞·ªõc:
    1. Thu th·∫≠p tin t·ª©c (news_service)
    2. L√†m s·∫°ch vƒÉn b·∫£n
    3. Tokenization (t·ª± ƒë·ªông trong analyzer)
    4. Embedding (t·ª± ƒë·ªông trong analyzer)
    5. D·ª± ƒëo√°n sentiment (Hybrid: keyword/FinBERT)
    6. Chuy·ªÉn v·ªÅ d·∫°ng s·ªë
    7. G·ªôp v√†o model
    """
    
    def __init__(self, use_finbert: bool = False):
        """
        Args:
            use_finbert: C√≥ load FinBERT kh√¥ng (t·ªën RAM)
        """
        logger.info("="*60)
        logger.info("üöÄ Enhanced Sentiment Pipeline v·ªõi Hybrid Analyzer")
        logger.info("="*60)
        
        self.analyzer = HybridSentimentAnalyzer(use_finbert=use_finbert)
        
        logger.info("‚úì Pipeline ready")
    
    def process_news_dataframe(self, news_df: pd.DataFrame, 
                               text_col: str = 'text') -> pd.DataFrame:
        """
        X·ª≠ l√Ω DataFrame tin t·ª©c
        
        Input: DataFrame v·ªõi columns [date, symbol, text, ...]
        Output: DataFrame with sentiment analysis
        """
        # Step 2: Clean (if needed)
        if 'text_clean' not in news_df.columns:
            from src.sentiment_pipeline import TextCleaner
            cleaner = TextCleaner()
            news_df['text_clean'] = news_df[text_col].apply(cleaner.clean)
            text_col = 'text_clean'
        
        # Steps 3-5: Analyze
        news_df = self.analyzer.analyze_dataframe(news_df, text_col=text_col)
        
        # Step 6: Aggregate by date
        daily_sentiment = self.analyzer.aggregate_by_date(news_df)
        
        return news_df, daily_sentiment
    
    def merge_with_price_data(self, price_df: pd.DataFrame, 
                             sentiment_df: pd.DataFrame) -> pd.DataFrame:
        """
        Step 7: Merge sentiment v√†o price data
        """
        logger.info("üîó Merging sentiment with price data")
        
        # Ensure date types
        price_df['date'] = pd.to_datetime(price_df['date'])
        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])
        
        # Determine merge keys
        merge_keys = ['date']
        if 'symbol' in price_df.columns and 'symbol' in sentiment_df.columns:
            merge_keys.append('symbol')
        
        # Merge
        merged = price_df.merge(sentiment_df, on=merge_keys, how='left')
        
        # Fill missing values
        sentiment_cols = [c for c in merged.columns if 'sentiment' in c or 'daily_' in c]
        for col in sentiment_cols:
            if merged[col].dtype in [np.float64, np.float32]:
                merged[col].fillna(0, inplace=True)
        
        if 'news_count' in merged.columns:
            merged[col].fillna(0, inplace=True)
        
        logger.info(f"‚úì Merged {len(merged)} rows")
        logger.info(f"  Sentiment coverage: {(merged.get('news_count', pd.Series([0])) > 0).sum() / len(merged) * 100:.1f}%")
        
        return merged


# ============ Test ============

def test_hybrid():
    """Test hybrid analyzer"""
    logger.info("\n" + "="*60)
    logger.info("üß™ TEST HYBRID SENTIMENT ANALYZER")
    logger.info("="*60)
    
    analyzer = HybridSentimentAnalyzer(use_finbert=False)
    
    test_cases = [
        "Vinamilk c√¥ng b·ªë l·ª£i nhu·∫≠n qu√Ω 3 tƒÉng 25% so v·ªõi c√πng k·ª≥",
        "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n s·ª•t gi·∫£m m·∫°nh do lo ng·∫°i l√£i su·∫•t",
        "FPT k√Ω h·ª£p ƒë·ªìng xu·∫•t kh·∫©u ph·∫ßn m·ªÅm 50 tri·ªáu USD",
        "Ng√¢n h√†ng c·∫£nh b√°o r·ªßi ro t√≠n d·ª•ng b·∫•t ƒë·ªông s·∫£n",
        "VIC ra m·∫Øt d·ª± √°n Vinhomes Ocean Park 3 quy m√¥ l·ªõn"
    ]
    
    logger.info("\nüìä Sentiment Analysis Results:")
    for i, text in enumerate(test_cases, 1):
        result = analyzer.analyze(text)
        
        logger.info(f"\n{i}. {text}")
        logger.info(f"   Sentiment: {result['sentiment'].upper()} ({result['method']})")
        logger.info(f"   Score: {result['sentiment_score']:.2f} (confidence: {result['confidence']:.2f})")
        logger.info(f"   Scores: pos={result['positive']:.2f}, neg={result['negative']:.2f}, neu={result['neutral']:.2f}")
        logger.info(f"   ‚Üí {result['explanation']}")


if __name__ == "__main__":
    test_hybrid()
