"""
News Service - Thu th·∫≠p v√† ph√¢n t√≠ch tin t·ª©c ch·ª©ng kho√°n TH·∫¨T
L·∫•y d·ªØ li·ªáu t·ª´ RSS feeds v√† web scraping t·ª´ c√°c ngu·ªìn uy t√≠n
"""

import re
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
from enum import Enum
import hashlib

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


class SentimentType(str, Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"


@dataclass
class NewsArticle:
    title: str
    summary: str
    url: str
    source: str
    published_at: str
    symbol: Optional[str] = None
    sentiment: SentimentType = SentimentType.NEUTRAL
    sentiment_score: float = 0.0
    impact_prediction: str = ""


class SentimentAnalyzer:
    """Ph√¢n t√≠ch sentiment tin t·ª©c ch·ª©ng kho√°n Vi·ªát Nam v·ªõi t·ª´ ƒëi·ªÉn m·ªü r·ªông"""
    
    POSITIVE_KEYWORDS = [
        # T√†i ch√≠nh - L·ª£i nhu·∫≠n
        "tƒÉng tr∆∞·ªüng", "l·ª£i nhu·∫≠n tƒÉng", "doanh thu tƒÉng", "v∆∞·ª£t k·∫ø ho·∫°ch",
        "c·ªï t·ª©c cao", "chia c·ªï t·ª©c", "tƒÉng v·ªën", "ph√°t h√†nh th√™m",
        "l√£i r√≤ng", "l√£i k·ª∑ l·ª•c", "tƒÉng m·∫°nh", "b·ª©t ph√°", "ƒë·ªôt ph√°",
        "tri·ªÉn v·ªçng t·ªët", "khuy·∫øn ngh·ªã mua", "m·ª•c ti√™u tƒÉng", "k·ª≥ v·ªçng",
        "l·∫°c quan", "t√≠ch c·ª±c", "kh·∫£ quan", "thu·∫≠n l·ª£i", "ho√†n th√†nh",
        "v∆∞·ª£t m·ª©c", "cao h∆°n k·ª≥ v·ªçng", "t·ªët h∆°n d·ª± b√°o",
        # Kinh doanh
        "m·ªü r·ªông", "ƒë·∫ßu t∆∞ m·ªõi", "h·ª£p t√°c", "k√Ω k·∫øt", "th·∫Øng th·∫ßu",
        "th√¢u t√≥m", "s√°p nh·∫≠p", "d·ª± √°n m·ªõi", "ra m·∫Øt", "xu·∫•t kh·∫©u tƒÉng", 
        "th·ªã ph·∫ßn tƒÉng", "kh√°ch h√†ng m·ªõi", "ƒë∆°n h√†ng m·ªõi", "h·ª£p ƒë·ªìng l·ªõn",
        "m·ªü th√™m", "khai tr∆∞∆°ng", "chi·∫øn l∆∞·ª£c", "ƒë·ªïi m·ªõi",
        # Th·ªã tr∆∞·ªùng
        "uptrend", "breakout", "v∆∞·ª£t ƒë·ªânh", "thanh kho·∫£n cao", "tƒÉng ƒëi·ªÉm",
        "kh·ªëi ngo·∫°i mua r√≤ng", "d√≤ng ti·ªÅn v√†o", "tƒÉng tr·∫ßn", "b·∫≠t tƒÉng",
        "h·ªìi ph·ª•c", "ph·ª•c h·ªìi", "ƒë·∫£o chi·ªÅu tƒÉng", "xanh", "s√°ng",
        "tƒÉng gi√°", "n√¢ng h·∫°ng", "thu h√∫t v·ªën",
        # ƒê√°nh gi√°
        "outperform", "overweight", "strong buy", "n√¢ng rating", "n√¢ng m·ª•c ti√™u",
        "v∆∞·ª£t k·ª≥ v·ªçng", "khuy·∫øn ngh·ªã", "ti·ªÅm nƒÉng", "c∆° h·ªôi",
    ]
    
    NEGATIVE_KEYWORDS = [
        # T√†i ch√≠nh - Thua l·ªó
        "thua l·ªó", "l·ªó r√≤ng", "gi·∫£m l·ª£i nhu·∫≠n", "doanh thu gi·∫£m",
        "n·ª£ x·∫•u", "n·ª£ tƒÉng", "ph√° s·∫£n", "gi·∫£i th·ªÉ", "m·∫•t v·ªën",
        "c·∫Øt c·ªï t·ª©c", "kh√¥ng chia c·ªï t·ª©c", "h·ªßy ni√™m y·∫øt", "tƒÉng v·ªën ·∫£o",
        "b·ªã ph·∫°t", "vi ph·∫°m", "gian l·∫≠n", "ƒëi·ªÅu tra", "thanh tra",
        "tƒÉng tr∆∞·ªüng √¢m", "s·ª•t gi·∫£m", "th·∫•t thu", "th·∫•t b·∫°i",
        "kh√¥ng ho√†n th√†nh", "th·∫•p h∆°n k·ª≥ v·ªçng", "k√©m d·ª± b√°o",
        # Kinh doanh
        "thu h·∫πp", "ƒë√≥ng c·ª≠a", "c·∫Øt gi·∫£m", "sa th·∫£i", "ng·ª´ng ho·∫°t ƒë·ªông",
        "m·∫•t h·ª£p ƒë·ªìng", "ki·ªán t·ª•ng", "tranh ch·∫•p", "ƒë√¨nh c√¥ng",
        "t·ªìn kho tƒÉng", "kh√°ch h√†ng r·ªùi b·ªè", "m·∫•t th·ªã ph·∫ßn",
        "t√°i c∆° c·∫•u", "c·∫Øt gi·∫£m nh√¢n s·ª±",
        # Th·ªã tr∆∞·ªùng
        "downtrend", "breakdown", "th·ªßng ƒë√°y", "m·∫•t ƒë√°y", "thanh kho·∫£n th·∫•p",
        "kh·ªëi ngo·∫°i b√°n r√≤ng", "d√≤ng ti·ªÅn ra", "gi·∫£m s√†n", "r∆°i t·ª± do",
        "b√°n th√°o", "c·∫Øt l·ªó", "panic sell", "th√°o ch·∫°y", "lao d·ªëc",
        "gi·∫£m m·∫°nh", "gi·∫£m s√¢u", "s·ª•p ƒë·ªï", "ƒë·ªè", "rung l·∫Øc",
        "gi·∫£m gi√°", "h·∫° h·∫°ng", "r√∫t v·ªën",
        # ƒê√°nh gi√°
        "underperform", "underweight", "sell", "h·∫° rating", "h·∫° m·ª•c ti√™u",
        "c·∫£nh b√°o", "r·ªßi ro cao", "k√©m k·ª≥ v·ªçng", "th·∫•t v·ªçng", "lo ng·∫°i",
    ]
    
    STRONG_MODIFIERS = ["k·ª∑ l·ª•c", "ƒë·ªôt bi·∫øn", "l·ªãch s·ª≠", "ch∆∞a t·ª´ng c√≥", "m·∫°nh nh·∫•t", "l·ªõn nh·∫•t", "cao nh·∫•t", "th·∫•p nh·∫•t"]
    
    def analyze(self, text: str) -> tuple:
        text_lower = text.lower()
        
        pos_count = sum(1 for kw in self.POSITIVE_KEYWORDS if kw in text_lower)
        neg_count = sum(1 for kw in self.NEGATIVE_KEYWORDS if kw in text_lower)
        
        has_strong = any(m in text_lower for m in self.STRONG_MODIFIERS)
        multiplier = 1.5 if has_strong else 1.0
        
        total = pos_count + neg_count
        if total == 0:
            return SentimentType.NEUTRAL, 0.0, "Kh√¥ng c√≥ t√≠n hi·ªáu r√µ r√†ng t·ª´ tin t·ª©c"
        
        score = ((pos_count - neg_count) / total) * multiplier
        score = max(-1.0, min(1.0, score))
        
        if score > 0.2:
            sentiment = SentimentType.POSITIVE
            impact = "üöÄ T√≠n hi·ªáu TƒÇNG M·∫†NH - Khuy·∫øn ngh·ªã MUA" if score > 0.6 else "üìà T√≠n hi·ªáu TƒÇNG - C√¢n nh·∫Øc mua v√†o"
        elif score < -0.2:
            sentiment = SentimentType.NEGATIVE
            impact = "üîª T√≠n hi·ªáu GI·∫¢M M·∫†NH - Khuy·∫øn ngh·ªã B√ÅN" if score < -0.6 else "üìâ T√≠n hi·ªáu GI·∫¢M - C√¢n nh·∫Øc c·∫Øt l·ªó"
        else:
            sentiment = SentimentType.NEUTRAL
            impact = "‚û°Ô∏è Trung l·∫≠p - Ti·∫øp t·ª•c theo d√µi di·ªÖn bi·∫øn"
        
        return sentiment, round(score, 2), impact


class NewsService:
    """Service thu th·∫≠p tin t·ª©c TH·∫¨T t·ª´ nhi·ªÅu ngu·ªìn RSS v√† Web"""
    
    # RSS Feeds ch·ª©ng kho√°n Vi·ªát Nam - Ngu·ªìn uy t√≠n (Updated 2024-12-03)
    # Ch·ªâ gi·ªØ l·∫°i c√°c feeds ƒëang ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh
    RSS_FEEDS = {
        # B√°o t√†i ch√≠nh chuy√™n ng√†nh - WORKING
        "VnEconomy_ChungKhoan": "https://vneconomy.vn/chung-khoan.rss",
        "VnEconomy_DoanhNghiep": "https://vneconomy.vn/doanh-nghiep.rss",
        "VnEconomy_TaiChinh": "https://vneconomy.vn/tai-chinh-ngan-hang.rss",
        
        # B√°o ch√≠nh th·ªëng - WORKING
        "TuoiTre_KinhDoanh": "https://tuoitre.vn/rss/kinh-doanh.rss",
        "ThanhNien_KinhTe": "https://thanhnien.vn/rss/kinh-te.rss",
        "VietnamNet_KinhDoanh": "https://vietnamnet.vn/rss/kinh-doanh.rss",
        "VnExpress_KinhDoanh": "https://vnexpress.net/rss/kinh-doanh.rss",
        "DanTri_KinhDoanh": "https://dantri.com.vn/kinh-doanh.rss",
        
        # C√°c feeds ƒë√£ t·∫Øt/l·ªói (ƒë·ªÉ tham kh·∫£o):
        # "CafeF": "https://cafef.vn/rss/chung-khoan.rss",  # 404 Error - ƒë·ªïi c·∫•u tr√∫c
        # "VTV": "https://vtv.vn/kinh-te.rss",  # 404 Error
        # "VietStock": "https://vietstock.vn/api/rss/cate/2",  # XML parse error
        # "NDH": "https://ndh.vn/rss/tai-chinh.rss",  # Connection timeout
    }
    
    # Mapping m√£ CK -> t·ª´ kh√≥a
    STOCK_KEYWORDS = {
        "VNM": ["vinamilk", "s·ªØa vi·ªát nam", "vnm", "s·ªØa vinamilk"],
        "VIC": ["vingroup", "t·∫≠p ƒëo√†n vin", "vic", "vinfast", "vin group"],
        "VHM": ["vinhomes", "vhm", "vin homes"],
        "VCB": ["vietcombank", "ng√¢n h√†ng ngo·∫°i th∆∞∆°ng", "vcb"],
        "BID": ["bidv", "ng√¢n h√†ng ƒë·∫ßu t∆∞", "bid"],
        "CTG": ["vietinbank", "ng√¢n h√†ng c√¥ng th∆∞∆°ng", "ctg"],
        "TCB": ["techcombank", "tcb", "techcom"],
        "MBB": ["mb bank", "mbbank", "qu√¢n ƒë·ªôi", "mbb", "mb"],
        "HPG": ["h√≤a ph√°t", "hoa phat", "th√©p h√≤a ph√°t", "hpg"],
        "MSN": ["masan", "msn", "t·∫≠p ƒëo√†n masan"],
        "FPT": ["fpt", "fpt corporation", "t·∫≠p ƒëo√†n fpt"],
        "MWG": ["th·∫ø gi·ªõi di ƒë·ªông", "ƒëi·ªán m√°y xanh", "mwg", "b√°ch h√≥a xanh", "mobile world"],
        "VPB": ["vpbank", "vp bank", "vpb"],
        "GAS": ["pvgas", "pv gas", "kh√≠ vi·ªát nam", "gas"],
        "SAB": ["sabeco", "bia s√†i g√≤n", "sab"],
        "PLX": ["petrolimex", "xƒÉng d·∫ßu", "plx"],
        "VJC": ["vietjet", "vjc", "vietjet air"],
        "SSI": ["ssi", "ch·ª©ng kho√°n ssi"],
        "VRE": ["vincom retail", "vre", "vincom"],
        "POW": ["pv power", "pow", "ƒëi·ªán l·ª±c d·∫ßu kh√≠"],
        "NVL": ["novaland", "nvl", "nova"],
        "ACB": ["acb", "√° ch√¢u", "ng√¢n h√†ng √° ch√¢u"],
        "STB": ["sacombank", "stb"],
        "TPB": ["tpbank", "ti√™n phong", "tpb"],
        "HDB": ["hdbank", "hdb", "ph√°t tri·ªÉn tphcm"],
        "VND": ["vndirect", "vnd", "ch·ª©ng kho√°n vndirect"],
        "GVR": ["cao su vi·ªát nam", "gvr", "rubber"],
        "BCM": ["becamex", "bcm"],
        "PDR": ["ph√°t ƒë·∫°t", "pdr"],
        "SHB": ["shb", "s√†i g√≤n h√† n·ªôi"],
    }
    
    def __init__(self):
        self.analyzer = SentimentAnalyzer()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
        }
        self._cache = {}
        self._cache_time = {}
        self._cache_duration = 300  # 5 ph√∫t
    
    def _get_cache_key(self, key: str) -> str:
        return hashlib.md5(key.encode()).hexdigest()
    
    def _is_cache_valid(self, key: str) -> bool:
        if key not in self._cache_time:
            return False
        return (datetime.now() - self._cache_time[key]).seconds < self._cache_duration
    
    def _clean_html(self, text: str) -> str:
        """Lo·∫°i b·ªè HTML tags"""
        if not text:
            return ""
        if HAS_BS4:
            return BeautifulSoup(text, 'html.parser').get_text()
        # Fallback n·∫øu kh√¥ng c√≥ BeautifulSoup
        clean = re.sub(r'<[^>]+>', '', text)
        clean = re.sub(r'&[a-zA-Z]+;', ' ', clean)
        return clean.strip()
    
    def _parse_date(self, date_str: str) -> str:
        """Parse nhi·ªÅu ƒë·ªãnh d·∫°ng ng√†y"""
        if not date_str:
            return datetime.now().strftime("%Y-%m-%d %H:%M")
        
        formats = [
            "%a, %d %b %Y %H:%M:%S %z",
            "%a, %d %b %Y %H:%M:%S GMT",
            "%a, %d %b %Y %H:%M:%S +0700",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%dT%H:%M:%SZ",
            "%Y-%m-%dT%H:%M:%S.%fZ",
            "%d/%m/%Y %H:%M",
            "%Y-%m-%d %H:%M:%S",
            "%d-%m-%Y %H:%M",
        ]
        
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str.strip(), fmt)
                return dt.strftime("%Y-%m-%d %H:%M")
            except:
                continue
        
        # Th·ª≠ parse ng√†y ti·∫øng Vi·ªát
        try:
            # "2 gi·ªù tr∆∞·ªõc", "30 ph√∫t tr∆∞·ªõc"
            if "gi·ªù tr∆∞·ªõc" in date_str.lower():
                hours = int(re.search(r'\d+', date_str).group())
                return (datetime.now() - timedelta(hours=hours)).strftime("%Y-%m-%d %H:%M")
            if "ph√∫t tr∆∞·ªõc" in date_str.lower():
                mins = int(re.search(r'\d+', date_str).group())
                return (datetime.now() - timedelta(minutes=mins)).strftime("%Y-%m-%d %H:%M")
            if "ng√†y tr∆∞·ªõc" in date_str.lower():
                days = int(re.search(r'\d+', date_str).group())
                return (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d %H:%M")
        except:
            pass
        
        return datetime.now().strftime("%Y-%m-%d %H:%M")
    
    def fetch_rss(self, feed_name: str, feed_url: str, limit: int = 15) -> List[NewsArticle]:
        """L·∫•y tin t·ª´ RSS feed"""
        cache_key = self._get_cache_key(f"rss_{feed_url}")
        
        if self._is_cache_valid(cache_key) and cache_key in self._cache:
            return self._cache[cache_key][:limit]
        
        news = []
        try:
            response = requests.get(feed_url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                # Parse XML
                try:
                    root = ET.fromstring(response.content)
                except ET.ParseError:
                    # Th·ª≠ clean content tr∆∞·ªõc khi parse
                    content = response.content.decode('utf-8', errors='ignore')
                    content = re.sub(r'&(?!amp;|lt;|gt;|quot;|apos;)', '&amp;', content)
                    root = ET.fromstring(content.encode('utf-8'))
                
                # T√¨m items trong RSS
                items = root.findall('.//item')
                if not items:
                    items = root.findall('.//{http://www.w3.org/2005/Atom}entry')
                
                for item in items[:limit * 2]:
                    try:
                        # L·∫•y title
                        title_el = item.find('title')
                        if title_el is None:
                            title_el = item.find('{http://www.w3.org/2005/Atom}title')
                        title = title_el.text if title_el is not None and title_el.text else ""
                        title = self._clean_html(title)
                        
                        if not title or len(title) < 10:
                            continue
                        
                        # L·∫•y description/summary
                        desc_el = item.find('description')
                        if desc_el is None:
                            desc_el = item.find('summary')
                        if desc_el is None:
                            desc_el = item.find('{http://www.w3.org/2005/Atom}summary')
                        
                        summary = ""
                        if desc_el is not None and desc_el.text:
                            summary = self._clean_html(desc_el.text)
                            summary = summary[:400] + "..." if len(summary) > 400 else summary
                        else:
                            summary = title[:200]
                        
                        # L·∫•y link
                        link_el = item.find('link')
                        if link_el is None:
                            link_el = item.find('{http://www.w3.org/2005/Atom}link')
                        
                        url = ""
                        if link_el is not None:
                            url = link_el.text if link_el.text else link_el.get('href', '')
                        
                        if not url:
                            continue
                        
                        # L·∫•y ng√†y
                        pub_el = item.find('pubDate')
                        if pub_el is None:
                            pub_el = item.find('published')
                        if pub_el is None:
                            pub_el = item.find('{http://www.w3.org/2005/Atom}published')
                        
                        date_str = self._parse_date(pub_el.text if pub_el is not None else "")
                        
                        # Ph√¢n t√≠ch sentiment
                        sentiment, score, impact = self.analyzer.analyze(title + " " + summary)
                        
                        news.append(NewsArticle(
                            title=title.strip(),
                            summary=summary.strip(),
                            url=url.strip(),
                            source=feed_name.replace("_", " "),
                            published_at=date_str,
                            sentiment=sentiment,
                            sentiment_score=score,
                            impact_prediction=impact
                        ))
                    except Exception as e:
                        continue
                
                # Cache k·∫øt qu·∫£
                if news:
                    self._cache[cache_key] = news
                    self._cache_time[cache_key] = datetime.now()
                
        except Exception as e:
            print(f"Error fetching RSS {feed_name}: {e}")
        
        return news[:limit]
    
    def scrape_cafef_web(self, limit: int = 10) -> List[NewsArticle]:
        """Scrape tin t·ª´ CafeF website (backup khi RSS kh√¥ng ho·∫°t ƒë·ªông)"""
        news = []
        if not HAS_BS4:
            return news
            
        try:
            # Scrape trang ch·ª©ng kho√°n CafeF
            url = "https://cafef.vn/chung-khoan.chn"
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # CafeF structure: t√¨m c√°c article items
                articles = soup.select('.tlitem, .item-news, .box-category-item')
                
                for article in articles[:limit]:
                    try:
                        # T√¨m title v√† link
                        title_link = article.select_one('h3 a, .title a, a[data-type="headline"]')
                        if not title_link:
                            continue
                        
                        title = title_link.get('title') or title_link.get_text(strip=True)
                        link = title_link.get('href', '')
                        
                        if not title or len(title) < 15:
                            continue
                        
                        # Fix relative URLs
                        if link and not link.startswith('http'):
                            link = 'https://cafef.vn' + link
                        
                        # T√¨m summary
                        summary_el = article.select_one('.sapo, .summary, .box-category-sapo, p')
                        summary = summary_el.get_text(strip=True) if summary_el else title[:200]
                        
                        # Sentiment analysis
                        sentiment, score, impact = self.analyzer.analyze(title + " " + summary)
                        
                        news.append(NewsArticle(
                            title=title.strip(),
                            summary=summary[:350],
                            url=link,
                            source="CafeF",
                            published_at=datetime.now().strftime("%Y-%m-%d %H:%M"),
                            sentiment=sentiment,
                            sentiment_score=score,
                            impact_prediction=impact
                        ))
                    except Exception as e:
                        continue
                        
        except Exception as e:
            print(f"CafeF scraping error: {e}")
        
        return news
    
    def filter_by_symbol(self, news: List[NewsArticle], symbol: str) -> List[NewsArticle]:
        """L·ªçc tin theo m√£ c·ªï phi·∫øu"""
        if not symbol:
            return news
        
        symbol = symbol.upper()
        keywords = self.STOCK_KEYWORDS.get(symbol, [])
        keywords.append(symbol.lower())
        
        filtered = []
        for article in news:
            text = (article.title + " " + article.summary).lower()
            if any(kw.lower() in text for kw in keywords):
                article.symbol = symbol
                filtered.append(article)
        
        return filtered
    
    def get_all_news(self, symbol: str = None, limit: int = 100) -> List[NewsArticle]:
        """L·∫•y tin t·ª´ T·∫§T C·∫¢ ngu·ªìn RSS v√† web scraping"""
        cache_key = self._get_cache_key(f"all_news_{symbol or 'general'}_{limit}")
        
        if self._is_cache_valid(cache_key) and cache_key in self._cache:
            return self._cache[cache_key]
        
        all_news = []
        
        # 1. L·∫•y t·ª´ t·∫•t c·∫£ RSS feeds (tƒÉng limit ƒë·ªÉ c√≥ nhi·ªÅu tin h∆°n)
        for feed_name, feed_url in self.RSS_FEEDS.items():
            try:
                news = self.fetch_rss(feed_name, feed_url, limit=20)  # Increased from 12 to 20
                all_news.extend(news)
            except Exception as e:
                print(f"Error with {feed_name}: {e}")
                continue
        
        # 2. Th·ª≠ scrape th√™m t·ª´ CafeF Web (v√¨ RSS ƒë√£ t·∫Øt)
        try:
            cafef_news = self.scrape_cafef_web(limit=15)  # Increased from 10 to 15
            all_news.extend(cafef_news)
        except Exception as e:
            print(f"CafeF scraping failed: {e}")
        
        # 3. Lo·∫°i b·ªè tr√πng l·∫∑p theo title
        seen_titles = set()
        unique_news = []
        for article in all_news:
            # Normalize title ƒë·ªÉ so s√°nh
            title_key = re.sub(r'[^\w\s]', '', article.title.lower())[:50]
            if title_key not in seen_titles and len(title_key) > 10:
                seen_titles.add(title_key)
                unique_news.append(article)
        
        # 4. L·ªçc theo symbol n·∫øu c√≥
        if symbol:
            symbol_news = self.filter_by_symbol(unique_news, symbol)
            
            # N·∫øu c√≥ √≠t tin ri√™ng, th√™m tin th·ªã tr∆∞·ªùng chung
            if len(symbol_news) < 10:
                general_news = [n for n in unique_news if n not in symbol_news]
                # ƒê√°nh d·∫•u tin chung
                for n in general_news:
                    if not n.symbol:
                        n.symbol = "MARKET"
                symbol_news.extend(general_news[:15 - len(symbol_news)])
            
            unique_news = symbol_news
        
        # 5. S·∫Øp x·∫øp theo th·ªùi gian (m·ªõi nh·∫•t tr∆∞·ªõc)
        unique_news.sort(key=lambda x: x.published_at, reverse=True)
        
        result = unique_news[:limit]
        
        # Cache k·∫øt qu·∫£
        self._cache[cache_key] = result
        self._cache_time[cache_key] = datetime.now()
        
        return result
    
    def get_sentiment_summary(self, symbol: str) -> Dict:
        """T·ªïng h·ª£p sentiment cho m·ªôt m√£ c·ªï phi·∫øu"""
        news = self.get_all_news(symbol, limit=30)
        
        if not news:
            return {
                "symbol": symbol,
                "overall": "neutral",
                "avg_score": 0,
                "positive_count": 0,
                "negative_count": 0,
                "neutral_count": 0,
                "total_news": 0,
                "recommendation": "Kh√¥ng c√≥ ƒë·ªß tin t·ª©c ƒë·ªÉ ph√¢n t√≠ch"
            }
        
        # T√≠nh s·ªë l∆∞·ª£ng t·ª´ng lo·∫°i
        pos = sum(1 for n in news if n.sentiment == SentimentType.POSITIVE)
        neg = sum(1 for n in news if n.sentiment == SentimentType.NEGATIVE)
        neu = sum(1 for n in news if n.sentiment == SentimentType.NEUTRAL)
        
        # T√≠nh ƒëi·ªÉm trung b√¨nh
        avg_score = sum(n.sentiment_score for n in news) / len(news)
        
        # X√°c ƒë·ªãnh xu h∆∞·ªõng t·ªïng th·ªÉ
        if avg_score > 0.25:
            overall = "positive"
            rec = f"üü¢ TIN T·ª®C T√çCH C·ª∞C ({pos}/{len(news)} tin t·ªët) - Xu h∆∞·ªõng thu·∫≠n l·ª£i, c√¢n nh·∫Øc MUA v√†o"
        elif avg_score < -0.25:
            overall = "negative"
            rec = f"üî¥ TIN T·ª®C TI√äU C·ª∞C ({neg}/{len(news)} tin x·∫•u) - C√¢n nh·∫Øc B√ÅN ho·∫∑c ch·ªù ƒë·ª£i th√™m"
        else:
            overall = "neutral"
            rec = f"üü° TIN T·ª®C TRUNG L·∫¨P - Th·ªã tr∆∞·ªùng ƒëang ch·ªù t√≠n hi·ªáu, theo d√µi th√™m"
        
        return {
            "symbol": symbol,
            "overall": overall,
            "avg_score": round(avg_score, 2),
            "positive_count": pos,
            "negative_count": neg,
            "neutral_count": neu,
            "total_news": len(news),
            "recommendation": rec
        }


# Singleton instance
news_service = NewsService()
